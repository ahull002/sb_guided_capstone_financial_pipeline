{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import date\n",
    "import pyspark.sql.functions as F \n",
    "\n",
    "# 3.1 Populate trade dataset\n",
    "# 3.1.1 Reading Trade partition dataset from its temporary location\n",
    "trade_common = spark.read.parquet(\"/HdiNotebooks/output_dir_csv_json/partition=T/*.parquet\")\n",
    "\n",
    "# 3.1.2 Selecting the Necessary columns for Trade Records\n",
    "trade = trade_common.select(\"trade_dt\", \"symbol\", \"exchange\", \"event_tm\",\"event_seq_nb\", \"arrival_tm\", \"trade_pr\")\n",
    "\n",
    "# 3.1.3 Apply Data Correction\n",
    "#In the exchange dataset, you can uniquely identify a record by the combination of trade_dt,\n",
    "#symbol, exchange, event_tm, event_seq_nb. However, the exchange may correct an error in\n",
    "#any submitted record by sending a new record with the same uniqueID. Such records will come with later arrival_tm. \n",
    "\n",
    "#Below code uses row_number and window partition and orderby to accept records with latest arrival_tme\n",
    "trade_corrected=trade.withColumn(\"row_number\",F.row_number().over(Window.partitionBy(trade.trade_dt,\\\n",
    "                   trade.symbol,trade.exchange,trade.event_tm,trade.event_seq_nb) \\\n",
    "                   .orderBy(trade.arrival_tm.desc()))).filter(F.col(\"row_number\")==1).drop(\"row_number\")\n",
    "\n",
    "# 3.1.4 Writing the Trade Dataset back to Azure Storage\n",
    "trade_date = \"2020-08-05\"\n",
    "trade_corrected.coalesce(1).write.parquet(\"wasbs://guidedsparkpro-2021-02-16t15-01-46-464z@guidedsparkprhdistorage.blob.core.windows.net/trade/trade_dt={}\".format(trade_date))\n",
    "\n",
    "\n",
    "# 3.2 Populate quote dataset using the same method in 3.1\n",
    "#########################################################\n",
    "#########################################################\n",
    "\n",
    "# 3.2.1 Reading the Quote partition dataset from its temporary location\n",
    "quote_common = spark.read.parquet(\"/HdiNotebooks/output_dir_csv_json/partition=Q/*.parquet\")\n",
    "\n",
    "# 3.2.2 Selecting the Necessary columns for Trade Records\n",
    "quote=quote_common.select(\"trade_dt\",\"symbol\",\"exchange\",\"event_tm\",\"event_seq_nb\",\"arrival_tm\",\"bid_pr\",\"ask_pr\")\n",
    "\n",
    "# 3.2.3\n",
    "#In the exchange dataset, you can uniquely identify a record by the combination of trade_dt,\n",
    "#symbol, exchange, event_tm, event_seq_nb. However, the exchange may correct an error in\n",
    "#any submitted record by sending a new record with the same uniqueID. Such records will come with later arrival_tm. \n",
    "\n",
    "#Below code uses row_number and window partition and orderby to accept records with latest arrival_tme\n",
    "\n",
    "quote_corrected=quote.withColumn(\"row_number\",F.row_number().over(Window.partitionBy(quote.trade_dt,quote.symbol,\\\n",
    "                                            quote.exchange,quote.event_tm,quote.event_seq_nb).\\\n",
    "                                            orderBy(quote.arrival_tm.desc()))).filter(F.col(\"row_number\")==1).drop(\"row_number\")\n",
    "\n",
    "# 3.2.4 Writing the quote Dataset back to Azure Storage\n",
    "### Writing back the Quote Dataset back to Azure Storage\n",
    "trade_date = \"2020-08-05\"\n",
    "quote.coalesce(1).write.parquet(\"wasbs://guidedsparkpro-2021-02-16t15-01-46-464z@guidedsparkprhdistorage.blob.core.windows.net/quote/trade_dt={}\".format(trade_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import date\n",
    "import pyspark.sql.functions as F \n",
    "\n",
    "# 3.1 Populate trade dataset\n",
    "# 3.1.1 Reading Trade partition dataset from its temporary location\n",
    "trade_common = spark.read.parquet(\"/HdiNotebooks/output_dir_csv_json/partition=T/*.parquet\")\n",
    "\n",
    "# 3.1.2 Selecting the Necessary columns for Trade Records\n",
    "trade = trade_common.select(\"trade_dt\", \"symbol\", \"exchange\", \"event_tm\",\"event_seq_nb\", \"arrival_tm\", \"trade_pr\")\n",
    "\n",
    "# 3.1.3 Apply Data Correction\n",
    "#In the exchange dataset, you can uniquely identify a record by the combination of trade_dt,\n",
    "#symbol, exchange, event_tm, event_seq_nb. However, the exchange may correct an error in\n",
    "#any submitted record by sending a new record with the same uniqueID. Such records will come with later arrival_tm. \n",
    "\n",
    "#Below code uses row_number and window partition and orderby to accept records with latest arrival_tme\n",
    "trade_corrected=trade.withColumn(\"row_number\",F.row_number().over(Window.partitionBy(trade.trade_dt,\\\n",
    "                   trade.symbol,trade.exchange,trade.event_tm,trade.event_seq_nb) \\\n",
    "                   .orderBy(trade.arrival_tm.desc()))).filter(F.col(\"row_number\")==1).drop(\"row_number\")\n",
    "\n",
    "# 3.1.4 Writing the Trade Dataset back to Azure Storage\n",
    "trade_date = \"2020-08-05\"\n",
    "trade_corrected.coalesce(1).write.parquet(\"wasbs://guidedsparkpro-2021-02-16t15-01-46-464z@guidedsparkprhdistorage.blob.core.windows.net/trade/trade_dt={}\".format(trade_date))\n",
    "\n",
    "\n",
    "# 3.2 Populate quote dataset using the same method in 3.1\n",
    "#########################################################\n",
    "#########################################################\n",
    "\n",
    "# 3.2.1 Reading the Quote partition dataset from its temporary location\n",
    "quote_common = spark.read.parquet(\"/HdiNotebooks/output_dir_csv_json/partition=Q/*.parquet\")\n",
    "\n",
    "# 3.2.2 Selecting the Necessary columns for Trade Records\n",
    "quote=quote_common.select(\"trade_dt\",\"symbol\",\"exchange\",\"event_tm\",\"event_seq_nb\",\"arrival_tm\",\"bid_pr\",\"ask_pr\")\n",
    "\n",
    "# 3.2.3\n",
    "#In the exchange dataset, you can uniquely identify a record by the combination of trade_dt,\n",
    "#symbol, exchange, event_tm, event_seq_nb. However, the exchange may correct an error in\n",
    "#any submitted record by sending a new record with the same uniqueID. Such records will come with later arrival_tm. \n",
    "\n",
    "#Below code uses row_number and window partition and orderby to accept records with latest arrival_tme\n",
    "\n",
    "quote_corrected=quote.withColumn(\"row_number\",F.row_number().over(Window.partitionBy(quote.trade_dt,quote.symbol,\\\n",
    "                                            quote.exchange,quote.event_tm,quote.event_seq_nb).\\\n",
    "                                            orderBy(quote.arrival_tm.desc()))).filter(F.col(\"row_number\")==1).drop(\"row_number\")\n",
    "\n",
    "# 3.2.4 Writing the quote Dataset back to Azure Storage\n",
    "### Writing back the Quote Dataset back to Azure Storage\n",
    "trade_date = \"2020-08-05\"\n",
    "quote.coalesce(1).write.parquet(\"wasbs://guidedsparkpro-2021-02-16t15-01-46-464z@guidedsparkprhdistorage.blob.core.windows.net/quote/trade_dt={}\".format(trade_date))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
